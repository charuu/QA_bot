{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b4fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --force-reinstall --no-cache-dir tenacity --user\n",
    "\n",
    "!pip install \"pypdf==4.2.0\" --user\n",
    "!pip install \"chromadb == 0.4.24\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce93ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio==4.44.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: ibm-watsonx-ai==1.1.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (1.1.2)\n",
      "Collecting langchain==0.2.11\n",
      "  Using cached langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community==0.2.10\n",
      "  Using cached langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-ibm==0.1.11 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (0.1.11)\n",
      "Requirement already satisfied: pypdf==4.3.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: pydantic==2.9.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (4.10.0)\n",
      "Requirement already satisfied: fastapi<1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.34.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (6.5.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (3.10.6)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (10.4.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.13.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.17.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (4.15.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (2.5.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio==4.44.0) (0.35.0)\n",
      "Requirement already satisfied: requests in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2.32.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2025.8.3)\n",
      "Requirement already satisfied: lomond in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (0.3.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (0.9.0)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2.13.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (8.7.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain==0.2.11) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain==0.2.11) (3.12.15)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain==0.2.11)\n",
      "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.11)\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.11)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain==0.2.11) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-community==0.2.10) (0.6.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic==2.9.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic==2.9.1) (2.23.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio-client==1.3.0->gradio==4.44.0) (2025.9.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from gradio-client==1.3.0->gradio==4.44.0) (12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from fastapi<1.0->gradio==4.44.0) (0.47.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx>=0.24.1->gradio==4.44.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.0) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (3.19.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio==4.44.0) (4.67.1)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (2.13.6)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (2.13.6)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (2.9.0.post0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.23->langchain==0.2.11) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from matplotlib~=3.0->gradio==4.44.0) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==4.44.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pandas<3.0,>=1.0->gradio==4.44.0) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from requests->ibm-watsonx-ai==1.1.2) (3.4.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.11) (3.2.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from typer<1.0,>=0.12->gradio==4.44.0) (14.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from importlib-metadata->ibm-watsonx-ai==1.1.2) (3.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from lomond->ibm-watsonx-ai==1.1.2) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio==4.44.0) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain==0.2.11) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (2.19.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (1.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (0.1.2)\n",
      "Using cached langchain-0.2.11-py3-none-any.whl (990 kB)\n",
      "Using cached langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
      "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Installing collected packages: langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.4.27\n",
      "    Uninstalling langsmith-0.4.27:\n",
      "      Successfully uninstalled langsmith-0.4.27\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.76\n",
      "    Uninstalling langchain-core-0.3.76:\n",
      "      Successfully uninstalled langchain-core-0.3.76\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.27\n",
      "    Uninstalling langchain-0.3.27:\n",
      "      Successfully uninstalled langchain-0.3.27\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.38\n",
      "    Uninstalling langchain-community-0.0.38:\n",
      "      Successfully uninstalled langchain-community-0.0.38\n",
      "Successfully installed langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.10 requires langchain-core<0.4.0,>=0.3.75, but you have langchain-core 0.2.43 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# installing necessary packages in my_env\n",
    "!python -m pip install \\\n",
    "gradio==4.44.0 \\\n",
    "langchain==0.2.11 \\\n",
    "langchain-community==0.2.10 \\\n",
    "pypdf==4.3.1 \\\n",
    "pydantic==2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a93313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (2.1.10)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (0.3.76)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-google-genai) (2.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.32.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (2.32.2)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.23.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.36.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (4.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (2.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (8.5.0)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\venv_langchain\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.36.0-py3-none-any.whl (244 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Installing collected packages: websockets, google-genai\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 12.0\n",
      "    Uninstalling websockets-12.0:\n",
      "      Successfully uninstalled websockets-12.0\n",
      "Successfully installed google-genai-1.36.0 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio-client 1.3.0 requires websockets<13.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain-google-genai langchain-core\n",
    "!pip install --upgrade google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264f0c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain-core 0.3.76\n",
      "Uninstalling langchain-core-0.3.76:\n",
      "  Successfully uninstalled langchain-core-0.3.76\n",
      "Found existing installation: langchain-google-genai 2.1.10\n",
      "Uninstalling langchain-google-genai-2.1.10:\n",
      "  Successfully uninstalled langchain-google-genai-2.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping langchain as it is not installed.\n",
      "WARNING: Skipping langchain-community as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.2.12\n",
      "  Using cached langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core==0.2.23\n",
      "  Using cached langchain_core-0.2.23-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-community==0.2.12\n",
      "  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-google-genai==1.0.7\n",
      "  Using cached langchain_google_genai-1.0.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain==0.2.12) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain==0.2.12) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain==0.2.12) (3.12.15)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested langchain-core==0.2.23\n",
      "    langchain 0.2.12 depends on langchain-core<0.3.0 and >=0.2.27\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install langchain-core==0.2.23 and langchain==0.2.12 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain langchain-core langchain-community langchain-google-genai\n",
    "!pip install langchain==0.2.12 langchain-core==0.2.23 langchain-community==0.2.12 langchain-google-genai==1.0.7\n",
    "!pip install --upgrade pydantic>=2.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e182868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai==1.0.7\n",
      "  Using cached langchain_google_genai-1.0.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting google-generativeai<0.8.0,>=0.7.0 (from langchain-google-genai==1.0.7)\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langchain-core<0.3,>=0.2.9 (from langchain-google-genai==1.0.7)\n",
      "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.6 (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7)\n",
      "  Using cached google_ai_generativelanguage-0.6.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.181.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (6.32.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (1.26.1)\n",
      "Collecting protobuf (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (1.74.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (4.9.1)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7)\n",
      "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.112 (from langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.9->langchain-google-genai==1.0.7) (1.3.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.30.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from tqdm->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai==1.0.7) (0.4.6)\n",
      "Downloading langchain_google_genai-1.0.7-py3-none-any.whl (36 kB)\n",
      "Downloading google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "   ---------------------------------------- 0.0/718.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 718.3/718.3 kB 14.7 MB/s  0:00:00\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, protobuf, packaging, langsmith, grpcio-status, langchain-core, google-ai-generativelanguage, google-generativeai, langchain-google-genai\n",
      "\n",
      "  Attempting uninstall: tenacity\n",
      "\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 6.32.0\n",
      "\n",
      "    Uninstalling protobuf-6.32.0:\n",
      "\n",
      "      Successfully uninstalled protobuf-6.32.0\n",
      "\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "  Attempting uninstall: packaging\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "    Found existing installation: packaging 25.0\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "    Uninstalling packaging-25.0:\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "      Successfully uninstalled packaging-25.0\n",
      "   ---- ----------------------------------- 1/9 [protobuf]\n",
      "   -------- ------------------------------- 2/9 [packaging]\n",
      "  Attempting uninstall: langsmith\n",
      "   -------- ------------------------------- 2/9 [packaging]\n",
      "    Found existing installation: langsmith 0.4.27\n",
      "   -------- ------------------------------- 2/9 [packaging]\n",
      "    Uninstalling langsmith-0.4.27:\n",
      "   -------- ------------------------------- 2/9 [packaging]\n",
      "      Successfully uninstalled langsmith-0.4.27\n",
      "   -------- ------------------------------- 2/9 [packaging]\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "  Attempting uninstall: grpcio-status\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "    Found existing installation: grpcio-status 1.74.0\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "    Uninstalling grpcio-status-1.74.0:\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "      Successfully uninstalled grpcio-status-1.74.0\n",
      "   ------------- -------------------------- 3/9 [langsmith]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "  Attempting uninstall: google-ai-generativelanguage\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "    Found existing installation: google-ai-generativelanguage 0.6.18\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "    Uninstalling google-ai-generativelanguage-0.6.18:\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "      Successfully uninstalled google-ai-generativelanguage-0.6.18\n",
      "   ---------------------- ----------------- 5/9 [langchain-core]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "  Attempting uninstall: google-generativeai\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "    Found existing installation: google-generativeai 0.8.5\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "    Uninstalling google-generativeai-0.8.5:\n",
      "   -------------------------- ------------- 6/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "      Successfully uninstalled google-generativeai-0.8.5\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ------------------------------- -------- 7/9 [google-generativeai]\n",
      "   ----------------------------------- ---- 8/9 [langchain-google-genai]\n",
      "   ---------------------------------------- 9/9 [langchain-google-genai]\n",
      "\n",
      "Successfully installed google-ai-generativelanguage-0.6.6 google-generativeai-0.7.2 grpcio-status-1.62.3 langchain-core-0.2.43 langchain-google-genai-1.0.7 langsmith-0.1.147 packaging-24.2 protobuf-4.25.8 tenacity-8.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-text-splitters 0.3.11 requires langchain-core<2.0.0,>=0.3.75, but you have langchain-core 0.2.43 which is incompatible.\n",
      "opentelemetry-proto 1.36.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai==1.0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cab6955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Machine learning is the use of algorithms that allow computer systems to learn from and make predictions on data without explicit programming.\\n' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-068be7ba-a7e2-44a0-858a-70258a0d6717-0' usage_metadata={'input_tokens': 9, 'output_tokens': 24, 'total_tokens': 33}\n"
     ]
    }
   ],
   "source": [
    "# Ensure compatible versions of langchain-google-genai and langchain-core\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\"\n",
    "    Load Google Gemini LLM for QA bot.\n",
    "    \"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",   # or \"gemini-1.5-flash\" for cheaper/faster\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = get_llm()\n",
    "response = llm.invoke(\"Explain what machine learning is in one sentence.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde21b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community==0.2.12\n",
      "  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (0.6.7)\n",
      "Collecting langchain<0.3.0,>=0.2.13 (from langchain-community==0.2.12)\n",
      "  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (0.2.43)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (0.1.147)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain-community==0.2.12)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-community==0.2.12) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.12) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12) (0.9.0)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.13->langchain-community==0.2.12)\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community==0.2.12) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community==0.2.12) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.2.12) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.2.12) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.12) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.12) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\charu\\repos\\xturing\\nlp-genai\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.12) (1.3.1)\n",
      "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.6/2.3 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 5.5 MB/s  0:00:00\n",
      "Using cached langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy, langchain-text-splitters, langchain, langchain-community\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.3.3\n",
      "\n",
      "    Uninstalling numpy-2.3.3:\n",
      "\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "    Found existing installation: langchain-text-splitters 0.3.11\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "    Uninstalling langchain-text-splitters-0.3.11:\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
      "   ---------------------------------------- 0/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   -------------------- ------------------- 2/4 [langchain]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ------------------------------ --------- 3/4 [langchain-community]\n",
      "   ---------------------------------------- 4/4 [langchain-community]\n",
      "\n",
      "Successfully installed langchain-0.2.17 langchain-community-0.2.12 langchain-text-splitters-0.2.4 numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community==0.2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21d876a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f98108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'Sample.pdf', 'page': 0}, page_content='A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large\\nmodels on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3\\nwith 175 billion parameters, fully fine-tuning them becomes computa-\\ntionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.\\nThis leads to up to 10,000 times fewer trainable parameters and reduces\\nGPU memory usage by three times. LoRA not only maintains but some-\\ntimes surpasses fine-tuning performance on models like RoBERTa, De-\\nBERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\\nno extra latency during inference, making it more efficient for practical\\napplications. All relevant code and model checkpoints are available at\\nhttps://github.com/microsoft/LoRA.\\n1 Introduction\\nMany natural language processing (NLP) applications rely on adapting large,\\npre-trained language models for various downstream tasks. Typically, this is\\ndone through fine-tuning, where all the parameters of the pre-trained model are\\nupdated. However, a significant drawback of fine-tuning is that the adapted\\nmodel has just as many parameters as the original one. As models grow in size,\\nwhat was once a manageable issue for models like GPT-2 or RoBERTa large\\nbecomes a serious deployment challenge with larger models like GPT-3, which\\nhas 175 billion trainable parameters.\\nTo mitigate these challenges, researchers have explored adapting only cer-\\ntain parts of the model or adding external modules specific to each task. This\\napproach reduces the need to store and manage large numbers of parameters\\nfor each task, greatly improving efficiency during deployment. However, cur-\\nrent methods often introduce drawbacks, such as inference delays by increasing\\n1'), Document(metadata={'source': 'Sample.pdf', 'page': 1}, page_content='model depth or reducing the usable sequence length. Furthermore, these meth-\\nods typically do not perform as well as full fine-tuning, leading to a trade-off\\nbetween efficiency and model performance.\\nInspired by prior works that demonstrate over-parametrized models often\\nreside in a low intrinsic dimensional space, we hypothesize that weight changes\\nduring model adaptation also have a low “intrinsic rank.” This insight leads to\\nour Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\\nposition matrices for the dense layers’ weight changes during adaptation, while\\nkeeping the pre-trained weights frozen. As illustrated in Figure 1, even with\\nlarge models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\\nmatrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\\nstorage and computation.\\nLoRA has several notable advantages:\\n•The pre-trained model can be shared, and small LoRA modules can be\\ncreated for various tasks. By freezing the main model and only switching\\nthe matrices AandB(shown in Figure 1), storage and task-switching\\noverhead are significantly reduced.\\n•LoRA improves training efficiency and reduces hardware requirements,\\nlowering the entry barrier by up to threefold when using adaptive opti-\\nmizers. This is because LoRA only requires updating the smaller low-rank\\nmatrices, avoiding the need to calculate gradients for most parameters.\\n•The simple linear design allows merging of the trainable matrices with\\nthe frozen pre-trained weights during deployment, ensuring no additional\\ninference latency compared to fully fine-tuned models.\\n•LoRA is compatible with many existing methods and can be combined\\nwith approaches like prefix-tuning.\\nIn this work, we follow standard conventions for Transformer architecture\\nand refer to dimensions such as dmodel, and projection matrices like Wq,Wk,Wv,\\nandWofor the self-attention module. WorW0represents a pre-trained weight\\nmatrix, while ∆ Wrefers to its update during adaptation. The rank rdenotes\\nthe rank of a LoRA module. Throughout, we use Adam for optimization and\\nmaintain the Transformer MLP feedforward dimension as dffn= 4×dmodel.\\n1.1 Key Advantages of LoRA\\n•Efficient Task Switching : A pre-trained model can support multiple\\ntasks by swapping the small LoRA matrices, reducing storage needs.\\n•Reduced Hardware Requirements : LoRA lowers the GPU memory\\nneeded for training by freezing most parameters and only training the\\nlow-rank matrices.\\n•No Additional Latency : LoRA incurs no extra inference delay because\\nthe matrices can be merged with the pre-trained weights when deployed.\\n2'), Document(metadata={'source': 'Sample.pdf', 'page': 2}, page_content='•Combining with Other Methods : LoRA can be used with other ap-\\nproaches, like prefix-tuning, to further optimize model performance.\\n2 Problem Statement\\nAlthough our approach is independent of the specific training objective, we focus\\non language modeling as the central application. Below, we outline the key\\naspects of the language modeling problem, particularly the goal of maximizing\\nconditional probabilities based on task-specific prompts.\\nAssume we have an autoregressive language model PΦ(y|x) that is pre-\\ntrained and parameterized by Φ. For example, PΦ(y|x) could be a general\\nmulti-task model such as GPT, built on top of the Transformer architecture.\\nThe model can then be adapted to different downstream tasks such as text\\nsummarization, machine reading comprehension (MRC), and natural language\\nto SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:\\nZ={(xi, yi)}i=1,...,N,\\nwhere both xiandyiare sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yiwould be the corresponding\\nSQL query; in summarization, xirepresents the article and yiwould be its\\nsummary.\\nIn traditional fine-tuning, the model is initialized using the pre-trained weights\\nΦ0, which are then updated to Φ 0+ ∆Φ by optimizing the model’s parameters\\nto maximize the conditional probabilities for each token:\\nmax\\nΦX\\n(x,y)∈Z|y|X\\nt=1log (PΦ(yt|x, y<t)) (1)\\nA significant limitation of full fine-tuning is that for every downstream task, a\\ndifferent set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the\\nsize of Φ 0. For large models, such as GPT-3 with 175 billion parameters, storing\\nand deploying multiple instances of fine-tuned models becomes impractical or\\nextremely challenging.\\nTo address this issue, we propose a more efficient approach where the task-\\nspecific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\\nof parameters Θ, where |Θ| ≪ |Φ0|. As a result, optimizing the model for each\\ntask reduces to optimizing Θ as follows:\\nmax\\nΘX\\n(x,y)∈Z|y|X\\nt=1log\\x00\\npΦ0+∆Φ(Θ) (yt|x, y<t)\\x01\\n(2)\\nIn the following sections, we explore a low-rank approach for representing\\n∆Φ, making the adaptation process more efficient in both computational and\\nmemory terms. For large models like GPT-3 175B, this method allows the\\ntrainable parameters |Θ|to be reduced to as little as 0.01% of |Φ0|.\\n3'), Document(metadata={'source': 'Sample.pdf', 'page': 3}, page_content='3 Limitations on Current Solutions\\nThe challenge we aim to address is not new. Since the rise of transfer learning,\\na great deal of work has focused on making model adaptation more efficient in\\nterms of both parameters and computation. For an overview, see Section 6 for\\nsome well-known works. Focusing on language modeling, two prominent strate-\\ngies for efficient adaptation stand out: adding adapter layers, or optimizing the\\ninput layer activations. However, both approaches come with limitations, espe-\\ncially when applied in large-scale, latency-sensitive production environments.\\n3.1 Adapter Layers and Inference Latency\\nThere are many variations of adapters. We focus on the original adapter design\\nfrom [ ?], which introduces two adapter layers per Transformer block, and a\\nmore recent approach by [ ?], which only uses one adapter per block but with an\\nadditional LayerNorm [ ?]. Although overall latency can be reduced by pruning\\nlayers or leveraging multi-task settings [ ?], [?], there is no way to completely\\neliminate the additional computation introduced by adapter layers. This might\\nseem minor since adapters generally have few parameters (typically less than\\n1% of the original model) due to their small bottleneck dimension, which limits\\nthe number of floating-point operations (FLOPs). However, large-scale neural\\nnetworks rely heavily on parallel processing to maintain low latency, and adapter\\nlayers are processed sequentially. This becomes more evident in scenarios with\\nlow batch sizes, such as real-time inference, where models like GPT-2 [ ?] running\\non a single GPU experience noticeable increases in latency, even with small\\nbottleneck dimensions (Table 1).\\nThe issue is further compounded when models need to be sharded across\\nmultiple devices, since the increased model depth requires more synchronous\\nGPU operations like AllReduce andBroadcast , unless adapter parameters are\\nredundantly replicated.\\n3.2 Challenges with Directly Optimizing the Prompt\\nAnother approach, such as prefix tuning [ ?], faces a different challenge. We\\nhave observed that prefix tuning is often difficult to optimize and that its per-\\nformance does not consistently improve as more trainable parameters are added,\\nconfirming earlier findings. Moreover, allocating part of the sequence length for\\nadaptation inevitably reduces the available sequence length for processing task-\\nrelated data, which seems to hinder prompt tuning’s performance compared to\\nother methods. We will further explore this issue in Section 5.\\n4'), Document(metadata={'source': 'Sample.pdf', 'page': 4}, page_content='Batch Size Sequence Length |Θ| Latency (ms)\\nFine-Tune/LoRA\\n32 512 0.5M 1449.4 ±0.8\\n16 256 11M 338.0±0.6\\n1 128 11M 19.8±2.7\\nAdapterL\\n32 512 0.5M 1482.0 ±1.0 (+2.2%)\\n16 256 11M 354.8±0.5 (+5.0%)\\n1 128 11M 23.9±2.1 (+20.7%)\\nAdapterH\\n32 512 0.5M 1492.2 ±1.0 (+3.0%)\\n16 256 11M 366.3±0.5 (+8.4%)\\n1 128 11M 25.8±2.2 (+30.3%)\\nTable 1: Inference latency of a forward pass in GPT-2 Medium measured over\\n100 trials using an NVIDIA Quadro RTX8000. ” |Θ|” refers to the number of\\ntrainable parameters in the adapter layers. AdapterLand AdapterHare two\\ntypes of adapter tuning. The impact on latency becomes significant, particularly\\nin online scenarios with shorter sequences and smaller batch sizes.\\n4 Our Method\\nIn this section, we explain the structure of LoRA and its practical benefits.\\nThe principles outlined here apply generally to dense layers in neural networks,\\nalthough we focus on specific weights in Transformer language models, as these\\nmodels serve as the central example in our experiments.\\n4.1 Low-Rank Parameterized Update Matrices\\nNeural networks contain numerous dense layers that perform matrix multipli-\\ncation, and the weight matrices in these layers typically have a full rank. When\\nadapting to a particular task, it shows that pre-trained language models pos-\\nsess a low ”intrinsic dimension” and can still perform effectively after a random\\nprojection to a smaller subspace. Drawing inspiration from this, we hypothesize\\nthat updates to the weights during adaptation also have a low ”intrinsic rank.”\\nFor a pre-trained weight matrix W0∈Rd×k, we limit its update by express-\\ning it as a low-rank decomposition, W0+ ∆W=W0+BA, where B∈Rd×r,\\nA∈Rr×k, and the rank r≪min(d, k). During training, W0is fixed, and A\\nandBare the trainable parameters. Both W0and ∆ W=BAare multiplied\\nwith the input, and their respective outputs are summed element-wise. Thus,\\nforh=W0x, our updated forward pass becomes:\\nh=W0x+ ∆Wx=W0x+BAx\\nWe illustrate this reparametrization in Figure 1. We initialize Awith random\\nGaussian values and set Bto zero, meaning ∆ W=BAis zero at the start\\n5'), Document(metadata={'source': 'Sample.pdf', 'page': 5}, page_content='of training. We then scale ∆ Wxbyα\\nr, where αis a constant dependent on\\nr. When using Adam for optimization, adjusting αhas an effect similar to\\ntuning the learning rate. Therefore, we use the same αfor our first experiments\\nand avoid tuning it. This scaling method also minimizes the need to adjust\\nhyperparameters when varying r.\\n4.1.1 A Generalization of Full Fine-Tuning\\nA more general fine-tuning technique involves training only a subset of pre-\\ntrained parameters. LoRA extends this approach by eliminating the need for\\nfull-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\\nmatrices for adaptation. If LoRA is applied to all weight matrices, and all\\nbiases are trained, the expressiveness of full fine-tuning is recovered by setting\\nthe LoRA rank requal to the rank of the pre-trained weight matrices. As the\\nnumber of trainable parameters increases, LoRA approaches the full fine-tuning\\nperformance, while adapter-based techniques converge to simpler models that\\ncannot process long input sequences.\\n4.1.2 No Additional Inference Latency\\nWhen deploying LoRA, we can explicitly compute W=W0+BAand use it\\nduring inference. This means that when switching between tasks, we can quickly\\nsubtract BAand add a different low-rank matrix B′A′without consuming ex-\\ntra memory. This ensures that no additional inference latency is introduced\\ncompared to fully fine-tuned models.\\n4.2 Applying LoRA to Transformer Models\\nIn principle, LoRA can be applied to any subset of weight matrices in a neural\\nnetwork to minimize the number of trainable parameters. In a Transformer\\narchitecture, the self-attention module contains four projection matrices Wq,\\nWk,Wv, and Wo, and the MLP module contains two more matrices. We treat\\nthe weight matrices in the self-attention module as a single dmodel×dmodel\\nmatrix, despite them being split into different attention heads. To simplify\\nthe process and improve parameter efficiency, we restrict our method to only\\nadapting the attention weights for downstream tasks, leaving the MLP module\\nfrozen. The effect of adapting various attention weight matrices in a Transformer\\nis further explored in Section 7.1. We leave the investigation of adapting MLP\\nlayers, LayerNorm layers, and biases to future work.\\n4.2.1 Practical Benefits and Limitations\\nOne of the major advantages of LoRA is its reduction in memory and storage\\ncosts. For large Transformer models using Adam, LoRA can cut VRAM usage\\nby up to two-thirds if r≪dmodel, since it eliminates the need to store optimizer\\nstates for frozen parameters. For example, with GPT-3 175B, VRAM usage\\nduring training drops from 1.2 TB to 350 GB. With r= 4, and only the query\\n6'), Document(metadata={'source': 'Sample.pdf', 'page': 6}, page_content='and value matrices being adapted, the checkpoint size decreases by approxi-\\nmately 10,000 ×(from 350 GB to 35 MB). This makes it possible to train using\\nsignificantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier\\ntask-switching during deployment by simply swapping out the LoRA weights,\\nwhich requires far less memory than loading entirely new model parameters.\\nAdditionally, LoRA offers a 25% training speedup compared to full fine-tuning\\nbecause there is no need to compute gradients for most parameters.\\nHowever, LoRA does have some limitations. It is not straightforward to\\ncombine multiple tasks with different low-rank matrices AandBin a single\\nforward pass if BAis absorbed into Wto remove additional inference latency.\\nWhile it is possible to dynamically select LoRA modules during inference, this\\nsolution is not suitable for scenarios where low-latency responses are crucial.\\n5 Empirical Experiments\\nWe assess LoRA’s performance in downstream tasks across several models in-\\ncluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\\nOur experiments cover various tasks, ranging from natural language understand-\\ning (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\\nwe evaluate on the GLUE benchmark. All experiments were performed using\\nNVIDIA Tesla V100 GPUs.\\n5.1 Baselines\\nFor comparison with a wide range of baselines, we replicate experimental setups\\nfrom previous studies and, where possible, reuse reported results. This might\\nresult in some baselines being present in only a subset of experiments.\\nFine-Tuning (FT) is a common method for adapting models. During fine-\\ntuning, the model’s pre-trained weights and biases are updated using gradient\\ndescent. A variant of this is fine-tuning only select layers, while freezing the\\nrest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,\\nwhile all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [ ?].\\nPrefix-embedding tuning (PreEmbed) involves adding special tokens to\\nthe input sequence, and training their embeddings. These tokens do not belong\\nto the model’s original vocabulary. Their placement—either prepended (prefix)\\nor appended (infix)—can significantly affect performance, as highlighted in [ ?].\\nPrefix-layer tuning (PreLayer) extends prefix tuning by learning train-\\nable activations at each Transformer layer. This results in a larger number of\\ntrainable parameters, as activations from prior layers are progressively replaced.\\nThe total number of trainable parameters is given by |Θ|=L×dmodel×(lp+li),\\nwhere Lis the number of Transformer layers.\\n7'), Document(metadata={'source': 'Sample.pdf', 'page': 7}, page_content='Adapter tuning [?] introduces additional fully connected adapter layers\\nbetween existing layers in the Transformer. Several variants exist, such as\\nAdapterH and AdapterL [ ?], which differ in the placement of adapters within\\nthe network. The number of trainable parameters in these methods is |Θ|=\\nLAdpt×(2×dmodel×r+r+dmodel) + 2×LLN×dmodel.\\nLoRA , on the other hand, introduces trainable low-rank matrices to the ex-\\nisting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank rand the shape of the original weight matrices:\\n|Θ|= 2×LLoRA×dmodel×r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.\\nTable 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher\\nscores are better for all metrics. Confidence intervals are provided for experi-\\nments we conducted. *Results from prior work.\\nModel & Method # Trainable Parameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (LoRA) 0.35M 70.4±0.1 8.85±0.2 46.8±0.2 71.8±0.1 2.53±0.2\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (LoRA) 0.77M 70.4±0.1 8.89±0.2 46.8±0.2 72.0±0.2 2.47±0.2\\n5.2 Scaling LoRA to GPT-3 175B\\nTo further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\\ncomputational cost of GPT-3, we only report standard deviations for each task\\nbased on multiple random seeds. See Appendix D.4 for hyperparameters used.\\nAs presented in Table ??, LoRA matches or outperforms full fine-tuning on\\nWikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\\ndo not consistently benefit from increasing the number of trainable parame-\\nters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\\nthe performance degradation seen with larger token embeddings in prefix-based\\nmethods.\\nFigure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-\\nters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\\nbetter scalability and performance.\\n8'), Document(metadata={'source': 'Sample.pdf', 'page': 8}, page_content='6 Related Works\\n6.1 Transformer Language Models\\nThe Transformer architecture, as introduced by Vaswani et al. (2017), has\\nproven to be a highly effective sequence-to-sequence model due to its heavy use\\nof self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\\nlanguage modeling, significantly boosting its utility in the field. Since then,\\nTransformer-based models have become a staple in natural language processing\\n(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\\nway for large-scale pre-trained language models that, when fine-tuned, deliver\\nexcellent performance on specific tasks. The next breakthrough came with GPT-\\n3 (Brown et al., 2020), which is currently the largest single Transformer language\\nmodel with 175 billion parameters.\\n6.2 Prompt Engineering and Fine-Tuning\\nDespite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\\ning), its performance is highly sensitive to how the input prompt is structured\\n(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process\\nthat involves crafting and fine-tuning the input prompts to maximize model per-\\nformance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin\\net al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing\\nfull fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\\nposes significant challenges due to the large memory requirements and the com-\\nputational resources needed, making it as resource-intensive as pre-training.\\n6.3 Parameter-Efficient Adaptation\\nMany techniques have been developed to address the inefficiency of full fine-\\ntuning by adapting only certain layers or introducing adapter modules. Houlsby\\net al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\\nadapter layers between existing layers in the network. These adapters allow for\\nparameter-efficient adaptation by learning only a small number of task-specific\\nparameters. Our method imposes a low-rank constraint on the weight updates,\\nensuring that learned weights can be merged with the main model weights dur-\\ning inference, thus introducing no additional latency, unlike the adapter layers.\\nA related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\\nproducts to parametrize the adapters, further improving parameter efficiency.\\nAdditionally, prompt optimization techniques, such as those proposed by Li and\\nLiang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\\noptimize the input tokens directly. However, these approaches typically reduce\\n9'), Document(metadata={'source': 'Sample.pdf', 'page': 9}, page_content='the available sequence length for task processing. Our work can be combined\\nwith such methods for further gains in efficiency.\\n6.4 Low-Rank Structures in Deep Learning\\nLow-rank structures are prevalent in many machine learning problems, and sev-\\neral studies have explored imposing these constraints on deep models. Li et\\nal. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\\nlearning tasks have an intrinsic low-rank structure. For deep neural networks,\\nparticularly over-parameterized models, it has been shown that they often ex-\\nhibit low-rank properties after training (Oymak et al., 2019). Prior works, such\\nas those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),\\nhave explicitly imposed low-rank constraints during training to enhance model\\nefficiency. However, our approach differs in that we apply low-rank updates to\\nfrozen pre-trained models, making it highly effective for task-specific adapta-\\ntion. Neural networks with low-rank structures have been shown to outperform\\nclassical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\\n2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in\\nadversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\\nposed low-rank adaptation well-grounded in both theory and practice.\\n7 Analyzing Low-Rank Adaptations\\nIn light of the demonstrated benefits of LoRA, we aim to further explore the\\nattributes of low-rank adaptation as applied to various downstream tasks. The\\nlow-rank structure does not only reduce the hardware requirements for conduct-\\ning parallel experiments, but it also provides better insight into how adapted\\nweights align with pre-trained weights. Our focus lies on GPT-3 175B, where\\nwe managed to significantly reduce the number of trainable parameters (up to\\n10,000 ×) without sacrificing task performance.\\nIn this section, we address some key questions:\\n•1)With a constrained parameter budget, which weight matrices should\\nbe adapted to achieve the best downstream task performance?\\n•2)Is the adapted matrix ∆ Wtruly rank-deficient, and if so, what rank is\\noptimal for practical use?\\n•3)How is ∆ Wrelated to the pre-trained weights W? Does ∆ Wexhibit\\nhigh correlation with W, and what is the comparative size of ∆ WtoW?\\nThe answers to these questions provide valuable insights for optimizing pre-\\ntrained models for downstream tasks.\\n7.1 Selecting Optimal Weight Matrices for LoRA\\nTo optimize performance under a limited parameter budget, we explore adapting\\ndifferent weight matrices within the self-attention module of the Transformer.\\n10'), Document(metadata={'source': 'Sample.pdf', 'page': 10}, page_content='We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\\n175B, using a rank r= 8 for one attention weight type or r= 4 for two types.\\nThe results are displayed in Table 3.\\nTable 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\\ndifferent attention weights in GPT-3, with a fixed number of trainable parame-\\nters.\\n# of Trainable Parameters = 18M WqWkWvWoWq, Wv\\nRank r= 8 70.4 70.0 73.0 73.2 73.7\\nMultiNLI ( ±0.1%) 91.0 90.8 91.0 91.3 91.7\\n7.2 Determining the Ideal Rank for LoRA\\nTo analyze the impact of the rank ron task performance, we applied LoRA with\\nvarying ranks across different combinations of attention matrices. The results\\ncan be found in Table 4.\\nTable 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\\nWeight Type r= 1 r= 2 r= 4 r= 8 r= 64\\nWikiSQL ( ±0.5%)Wq68.8 69.6 70.5 70.4 70.0\\nWikiSQL ( Wq, Wv) 73.4 73.3 73.7 73.8 73.5\\nMultiNLI ( ±0.1%)Wq90.7 90.9 91.1 90.7 90.7\\nMultiNLI ( Wq, Wv) 91.3 91.4 91.3 91.6 91.4\\nThe results show that even at a small rank r= 1, LoRA performs well when\\nboth WqandWvare adapted. In contrast, adapting only Wqrequires a higher\\nrank for optimal performance.\\n8 Conclusion\\nLoRA offers a highly efficient solution to the problem of adapting large language\\nmodels for downstream tasks. By freezing the majority of the model’s param-\\neters and training only small, low-rank matrices, LoRA achieves comparable\\nperformance to full fine-tuning while drastically reducing computational costs.\\nIts ability to scale to massive models like GPT-3 without sacrificing performance\\nhighlights its potential for widespread use.\\nFuture work could explore combining LoRA with other parameter-efficient\\nmethods or investigating more principled ways to select which weight matrices\\nto adapt. Additionally, further studies on the rank deficiency of pre-trained\\nweights could inspire new developments in efficient model adaptation.\\n11')]\n"
     ]
    }
   ],
   "source": [
    "## Document loader\n",
    "def document_loader(file):\n",
    "    loader = PyPDFLoader(file)\n",
    "    loaded_document = loader.load()\n",
    "    return loaded_document\n",
    "\n",
    "doc= document_loader(\"Sample.pdf\")\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a53093af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Text splitter\n",
    "def text_splitter(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "\n",
    "chunks= text_splitter(doc)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e88deebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charu\\AppData\\Local\\Temp\\ipykernel_1680\\1806874834.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embeddings():\n",
    "    \"\"\"\n",
    "    Load Hugging Face embedding model for vector database / retrieval.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # small & fast\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embedding_model\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "160582ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "778ecd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:412: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000015A98BE5340>\n"
     ]
    }
   ],
   "source": [
    "## Vector db\n",
    "def vector_database(chunks):\n",
    "    embedding_model = get_embeddings()\n",
    "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
    "    return vectordb\n",
    "\n",
    "db = vector_database(chunks)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22e3af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000015A9FADB680>\n"
     ]
    }
   ],
   "source": [
    "## Retriever\n",
    "def retriever(file):\n",
    "    splits = document_loader(file)\n",
    "    chunks = text_splitter(splits)\n",
    "    vectordb = vector_database(chunks)\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "ret =retriever(\"Sample.pdf\")\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e98d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic of the provided text is the relationship between `xi` (representing an article) and `yi` (representing its summary) in a SQL query context.  The text repeatedly states this relationship.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## QA Chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def retriever_qa(file, query):\n",
    "    llm = get_llm()\n",
    "    retriever_obj = retriever(file)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=retriever_obj, \n",
    "                                    return_source_documents=True)\n",
    "    response = qa.invoke({\"query\": query})\n",
    "\n",
    "    return response['result']\n",
    "\n",
    "response = retriever_qa(\"Sample.pdf\", \"What is the main topic of the document?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "137f2e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create Gradio interface\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m      4\u001b[39m rag_application = gr.Interface(\n\u001b[32m      5\u001b[39m     fn=retriever_qa,  \n\u001b[32m      6\u001b[39m     allow_flagging=\u001b[33m\"\u001b[39m\u001b[33mnever\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mUpload a PDF document and ask any question. The chatbot will try to answer using the provided document.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Launch the app\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_simple_templates\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\_simple_templates\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimpledropdown\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimpleimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImage\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimpletextbox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\_simple_templates\\simpledropdown.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Sequence\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Component, FormComponent\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\components\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mannotated_image\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnnotatedImage\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     Component,\n\u001b[32m      5\u001b[39m     FormComponent,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     get_component_instance,\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\components\\annotated_image.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m handle_file\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocumentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m document\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m processing_utils, utils\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Component\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m client_utils\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, ImageSequence, PngImagePlugin\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, wasm_utils\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel, GradioRootModel, JsonData\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\utils.py:58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blocks_context\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     59\u001b[39m     BlocksConfigDict,\n\u001b[32m     60\u001b[39m     DeveloperPath,\n\u001b[32m     61\u001b[39m     FileData,\n\u001b[32m     62\u001b[39m     UserProvidedPath,\n\u001b[32m     63\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error, InvalidPathError\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m en\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\gradio\\data_classes.py:23\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum, auto\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     Any,\n\u001b[32m     13\u001b[39m     Iterator,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     Union,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Request\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocumentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m document\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m traverse\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\fastapi\\__init__.py:7\u001b[39m\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.116.1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstarlette\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m status \u001b[38;5;28;01mas\u001b[39;00m status\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastAPI \u001b[38;5;28;01mas\u001b[39;00m FastAPI\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackground\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackgroundTasks \u001b[38;5;28;01mas\u001b[39;00m BackgroundTasks\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatastructures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UploadFile \u001b[38;5;28;01mas\u001b[39;00m UploadFile\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\fastapi\\applications.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     Any,\n\u001b[32m      4\u001b[39m     Awaitable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     Union,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m routing\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatastructures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Default, DefaultPlaceholder\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception_handlers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     http_exception_handler,\n\u001b[32m     20\u001b[39m     request_validation_exception_handler,\n\u001b[32m     21\u001b[39m     websocket_request_validation_exception_handler,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\fastapi\\routing.py:25\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum, IntEnum\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Any,\n\u001b[32m     10\u001b[39m     AsyncIterator,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     Union,\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m params\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     ModelField,\n\u001b[32m     28\u001b[39m     Undefined,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     lenient_issubclass,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatastructures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Default, DefaultPlaceholder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\fastapi\\params.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Sequence, Union\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Example\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfields\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Annotated, deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\fastapi\\openapi\\models.py:124\u001b[39m\n\u001b[32m    120\u001b[39m     description: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    121\u001b[39m     url: AnyUrl\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSchema\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModelWithConfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Ref: JSON Schema 2020-12: https://json-schema.org/draft/2020-12/json-schema-core.html#name-the-json-schema-core-vocabu\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Core Vocabulary\u001b[39;49;00m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malias\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$vocabulary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\pydantic\\main.py:286\u001b[39m, in \u001b[36m__new__\u001b[39m\u001b[34m(mcs, name, bases, namespace, **kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\pydantic\\main.py:808\u001b[39m, in \u001b[36m__try_update_forward_refs__\u001b[39m\u001b[34m(cls, **localns)\u001b[39m\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__pydantic_core_schema__\n\u001b[32m    803\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handler(source)\n\u001b[32m    805\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__get_pydantic_json_schema__\u001b[39m(\n\u001b[32m    807\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m     core_schema: CoreSchema,\n\u001b[32m    809\u001b[39m     handler: GetJsonSchemaHandler,\n\u001b[32m    810\u001b[39m     /,\n\u001b[32m    811\u001b[39m ) -> JsonSchemaValue:\n\u001b[32m    812\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Hook into generating the model's JSON schema.\u001b[39;00m\n\u001b[32m    813\u001b[39m \n\u001b[32m    814\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    827\u001b[39m \u001b[33;03m        A JSON schema, as a Python object.\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handler(core_schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\pydantic\\typing.py:554\u001b[39m, in \u001b[36mupdate_model_forward_refs\u001b[39m\u001b[34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\pydantic\\typing.py:520\u001b[39m, in \u001b[36mupdate_field_forward_refs\u001b[39m\u001b[34m(field, globalns, localns)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charu\\repos\\xTuring\\NLP-GENAI\\.venv\\Lib\\site-packages\\pydantic\\typing.py:66\u001b[39m, in \u001b[36mevaluate_forwardref\u001b[39m\u001b[34m(type_, globalns, localns)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mTypeError\u001b[39m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "rag_application = gr.Interface(\n",
    "    fn=retriever_qa,  \n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),  # Drag and drop file upload\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"chatbot\",\n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "\n",
    "rag_application.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f7880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
